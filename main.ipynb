{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import datetime\n",
    "from torchsummary import summary\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(device)\n",
    "latent_size = 64\n",
    "hidden_size = 256\n",
    "image_size = 28**2\n",
    "num_epochs = 300\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "sample_dir = 'my_samples3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "                # transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))]) # 3 for RGB channels\n",
    "\n",
    "# MNIST dataset\n",
    "mnist = torchvision.datasets.MNIST(root='../',\n",
    "                                   train=True,\n",
    "                                   transform=transform,\n",
    "                                   download=False)\n",
    "mnist_test = torchvision.datasets.MNIST(root='../',\n",
    "                                   train=False,\n",
    "                                   transform=transform,\n",
    "                                   download=False)\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=False,\n",
    "                                               pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in data_loader:\n",
    "#     print(len(x))\n",
    "# #    print(x[0].shape)\n",
    "#     #print(x.shape)\n",
    "#  #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,*k,**k1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            #1 * 28 * 28\n",
    "            nn.Conv2d(1,4,5), # 4 * 24 * 24\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3), # 4 * 8 * 8\n",
    "            nn.Conv2d(4,16,5,padding=1,stride=2), # 16 * 3 * 3\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3) # 16 * 1 * 1\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0],1,28,28)\n",
    "        y = self.conv(x)\n",
    "        y = y.view(y.shape[0],-1)\n",
    "        y = self.fc(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_channel = 16\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,*k,**k1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_size,feature_channel*4,4,bias=False), # 64 * 4 * 4\n",
    "            nn.BatchNorm2d(feature_channel*4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(feature_channel*4,feature_channel*2,7,2,1,bias=False),  # 32  * 11 * 11\n",
    "            nn.BatchNorm2d(feature_channel*2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(feature_channel*2,feature_channel,7,2,1,bias=False),  # 16  * 25 * 25\n",
    "            nn.BatchNorm2d(feature_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(feature_channel,1,4,bias=False),  # 1  * 28 * 28\n",
    "            nn.Tanh(),  #激活函数\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.main(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netG = Generator(latent_size,hidden_size,28**2)\n",
    "netD = Discriminator(28**2,256,1)\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netG.to(device)\n",
    "netD.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adadelta(netD.parameters())\n",
    "optimizerG = optim.Adadelta(netG.parameters())\n",
    "# optimizerD = torch.optim.Adam(netD.parameters(), lr=lr)\n",
    "# optimizerG = torch.optim.Adam(netG.parameters(), lr=lr)\n",
    "\n",
    "# netG.cuda()\n",
    "summary(netG,input_size=(latent_size,1,1))\n",
    "summary(netD,input_size=(1,28,28))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(netG,netD,num_epochs,optG,optD,data_loader,test_data_loader,criterion):\n",
    "    \n",
    "    for one in data_loader:\n",
    "        temp = denorm(one[0])\n",
    "        save_image(temp,os.path.join(sample_dir,'real.png'))\n",
    "        break\n",
    "    for epoch in range(num_epochs):\n",
    "        with torch.no_grad():\n",
    "            fake_images = netG(torch.randn((batch_size,latent_size,1,1),device=device))\n",
    "            fake_images = fake_images.reshape(fake_images.shape[0],1,28,28)\n",
    "            fake_images = denorm(fake_images)\n",
    "            save_image(fake_images,os.path.join(sample_dir,'fake_images-{}.png'.format(epoch)))\n",
    "            \n",
    "        for i,data in enumerate(data_loader,0):\n",
    "            netD.zero_grad()\n",
    "            \n",
    "            \n",
    "            real_pic = data[0].to(device)\n",
    "            b_size = real_pic.size(0)\n",
    "            \n",
    "            noise = torch.randn((b_size,latent_size,1,1),device=device)\n",
    "            fake_pic = netG(noise)\n",
    "            \n",
    "            real_label = torch.full((b_size,),1,dtype=torch.float,device=device)\n",
    "            fake_label = torch.full((b_size,),0,dtype=torch.float,device=device)\n",
    "            \n",
    "            output1 = netD(real_pic).view(-1)\n",
    "            output2 = netD(fake_pic.detach()).view(-1)\n",
    "       #     print(output.shape,label.shape)\n",
    "            errD_real = criterion(output1,real_label)\n",
    "            errD_fake = criterion(output2,fake_label)\n",
    "            errD_sum = errD_real + errD_fake\n",
    "            errD_sum.backward()\n",
    "            \n",
    "            D_x = output1.mean().item()\n",
    "            D_G_z1 = output2.mean().item()\n",
    "            \n",
    "            optD.step()\n",
    "\n",
    "            netG.zero_grad()\n",
    "            output = netD(fake_pic).view(-1)\n",
    "            errG = criterion(output,real_label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            \n",
    "            optG.step()\n",
    "            if i % 600 == 0:\n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(data_loader),\n",
    "                     errD_sum.item(), errG.item(), D_x, D_G_z1, D_G_z2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "train(netG,netD,num_epochs,optimizerG,optimizerD,data_loader,test_data_loader,criterion)\n",
    "\n",
    "print(datetime.datetime.now()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
